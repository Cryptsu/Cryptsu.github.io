---
title: "BRICS+ CTF - SSS: Optimizing FFT"
date: Tue Oct 04 2023 15:28:12 GMT+0700 (Indochina Time)
description: I'm depressed after trying to solve this...
draft: true
---

# Intro

Basically, the challenge:
- generates a random polynomial $f(x)$ with $N=$`20000156` terms in $\mathbb{F}_{p}$, where $p = 20000159$,
- evaluates the outputs $f(0), f(1), ..., f(p-1)$ except for $f(69), f(420)$ and $f(1337)$,
- gives us said outputs,
- gives us the `FLAG` **XORed** with the `sha512()` hash of $f(x)$'s coefficients array.
<br/>

# Trying stuffs...

Since it's hard to crack `sha512()`, or reverse **XOR** function with 0 knowleges about one of its inputs, our only route is to recover $f(x)$ from its outputs.

## Lagrange Interpolation

So, if you're a noob like me, your first thought will probably be: 
> "Well, this is a piece of cake. 
> 
> We have $20000156$ outputs, the polynomial has $20000156$ terms. I'll just use the [Lagrange Interpolation](https://www.wikiwand.com/en/Lagrange_polynomial). They have this fancy math that **computes a polynomial's coefficients from its evaluations**. 
> 
> We meet all the requirements needed to carry out the computation."
<br/>

So you gleefully copy an implementation from the Internet, adding inputs, modify stuffs, typing, thinking to yourself: **"EZ"**. 

You run the code and wonder why it takes so long. Maybe the output array is too big and needed some time to load? No, it had already been done! You throw a `tqdm` bar into the main loop of the code and see that it estimates the algorithm will be running for another `7000` hours.
<br/>

Shit.

You dig into the implementation of **Lagrange Interpolation**, and realize that given a polynomial with $N$ terms:
- it computes the sum of $N$ terms, 
- each term is the product of smaller $N-1$ linear polynomials.
<br/>

An $O(N^2)$ runtime... 

This algorithm works fast for small polynomial sizes, like `2` or `3` and some numbers close to it like `10000`, but not `20000156`. This is when you gasp: *"This is not Crypto, this is some [Codeforces](https://codeforces.com/) obsure **200+ IQ** optimization problem!!"* You know you suck, but you still want try another stuffs anyways, to see where it goes.
<br/>

## The Vandermonde Matrix
From your experiences of watching math videos on Youtube, feeling smart then remember like **1%** about it the next day, you realize that ~~you don't know what to do now and feel like shit *(meanwhile, your teammate is saying something about **vandermonde**)*~~ maybe sometimes we need to derail to some random ideas to get to smart one, so that's what you did.
<br/>

So you definitely ~~did look at [this wiki page](https://www.wikiwand.com/en/Vandermonde_matrix) and~~ have **an insight** on your own ~~*(no)*~~: 

The evaluation of $f(x) = \sum_{k=0}^{N-1} f_k x^k$ at $x_0$ can be represented as a dot product: $f(x_0) = (x_0^0, x_0^1, ..., x_0^{N-1}) \cdot (f_0, f_1, ..., f_{N-1})$.
<br/>

Or maybe, as a multiplication of `1xN` matrix with another `Nx1` matrix?
$$
\begin{bmatrix}
x_0^0 & x_0^1 & ... & x_0^{N-1}
\end{bmatrix}

\begin{bmatrix}
f_0 \\ f_1 \\ ... \\ f_{N-1}
\end{bmatrix}

= 

\begin{bmatrix}
f(x_0)
\end{bmatrix}
$$
<br/>

And you see that you can extend the `1xN` matrix on the left so that on the right side you can compute not only $f(x_0)$, but also $f(x_1), f(x_2), ...$ and so on:
$$
\begin{bmatrix}
x_0^0     & x_0^1     & ... & x_0^{N-1}     \\
x_1^0     & x_1^1     & ... & x_1^{N-1}     \\
... \\
x_{N-1}^0 & x_{N-1}^1 & ... & x_{N-1}^{N-1} \\
\end{bmatrix}

\begin{bmatrix}
f_0 \\ f_1 \\ ... \\ f_{N-1}
\end{bmatrix}

= 

\begin{bmatrix}
f(x_0) \\ f(x_1) \\ ... \\ f(x_{N-1})
\end{bmatrix}
$$
<br/>

The `NxN` matrix on the left side seems like a special type of matrix. Maybe you discovered a new thing ðŸ˜± (and not something already been discovered and called [the Vandermonde Matrix](https://www.wikiwand.com/en/Vandermonde_matrix))? Anyways, you decide to name this thing [the Vandermonde Matrix](https://www.wikiwand.com/en/Vandermonde_matrix), because it sounds right somehow...

Let's rewrite the equation into: 
$
V \text{ } \overrightarrow{f} = \overrightarrow{y}
$,
where:
- $\overrightarrow{f}$ is $f(x)$'s coefficients vector.
- $\overrightarrow{y}$ is $f(x)$'s outputs vector.
- $V$ is the Vandermonde matrix.
<br/>

Which leads to $\overrightarrow{f} = V^{-1} \text{ } \overrightarrow{y}$. As long as $x_0 \neq x_1 \neq x_2 \neq ... \neq x_{N-1}$, $V$ is invertible. Anyways, this shifts our focus into...

![this is when I started googling about vandermonde](genni-insight.png)

A couple of Google Searches leads us to [this website](https://proofwiki.org/wiki/Inverse_of_Vandermonde_Matrix):

![seems bad, but I dunno why...](inverse_vandermonde.png)

You have a hunch that this route sucks. Matrix multiplication with a vector still takes $O(N^2)$ as far as you know, and at the same time, `EggRoll`, from your team says this:

![](eggroll-insight.png)

Others seem to brush off his idea, and to be honest, they seem tired and confused at that point, so it's understandable. But you remember from the **1%** of knowledge you watched in some Youtube video, that **FFT** is some algorithm that can be done in $O(N\log{N})$, which is something way better than $O(N^2)$, is probably exactly what we need right now!! 
<br/>

Hmm... But you forgot what it does...
<br/>

## Fast Fourier Transform

**Fast Fourier Transform**, or **FFT** for short, is an algorithm that also can do the **convert coefficients -> values** thingy. However, this algorithm has a restriction: 

The inputs to $f(x)$ must be $\omega_N^0, \omega_N^1, ..., \omega_N^{N-1}$, instead of arbitrary values $x_0, x_1, ..., x_{N-1}$, where $\omega_N$ is **any** $N$-**th [root of unity](https://www.wikiwand.com/en/Root_of_unity).** 
<br/>

> Hmm.. The actual purpose of **FFT** seems to be **some values -> frequency thingy conversion** stuffs. But in this case, it still true **(?)** idk... math is weird... But if you want to know the actual definition of **FFT**, you can read it here: [Wiki](https://www.wikiwand.com/en/Fast_Fourier_transform)
<br/>

To understand why **FFT** does this, let's formulize things.

Same as before, this relationship between coefficients and values can be represented as the product of a **Vandermonde matrix** with the **coefficient vector** $\overrightarrow{f}$:

$$
\begin{bmatrix}
    \omega_N^0 & \omega_N^0     & \omega_N^0        & ... & \omega_N^0            & \omega_N^0             \\
    \omega_N^0 & \omega_N^1     & \omega_N^2        & ... & \omega_N^{N-2}        & \omega_N^{N-1}         \\
    \omega_N^0 & \omega_N^2     & \omega_N^4        & ... & \omega_N^{2N-4}       & \omega_N^{2N-2}       \\
    ... \\
    \omega_N^0 & \omega_N^{N-1} & \omega_N^{2(N-1)} & ... & \omega_N^{(N-2)(N-1)} & \omega_N^{(N-1)(N-1)} \\
\end{bmatrix}

\begin{bmatrix}
f_0     \\
f_1     \\
f_2     \\
...     \\
f_{N-2} \\
f_{N-1}
\end{bmatrix}

=

\begin{bmatrix}
f(\omega_N^0) \\
f(\omega_N^1) \\
f(\omega_N^2) \\
... \\
f(\omega_N^{N-2}) \\
f(\omega_N^{N-1})
\end{bmatrix}
$$
<br/>

Let's rewrite the equation into: 
$
D_{\omega_N} \text{ } \overrightarrow{f} = \overrightarrow{y_{\omega_N}}
$, where:
- $\overrightarrow{f}$ is $f(x)$'s coefficients vector.
- $\overrightarrow{y_{\omega_N}}$ is $f(x)$'s outputs vector of $f(x)$ evaluated at $\omega_N^0, \omega_N^1, ..., \omega_N^{N-1}$.
- $D_{\omega_N}$ is a special case of Vandermonde matrix, called the [DFT matrix](https://www.wikiwand.com/en/DFT_matrix), whose entries can be generated solely from $\omega_N$.
<br/>

Same as before, we have: $\overrightarrow{f} = D_{\omega_N}^{-1} \text{ } \overrightarrow{y_{\omega_N}}$. And it turns out that,
$$
D_{\omega_N}^{-1} = \frac{D_{\omega_N^{-1}}}{N}
$$
<br/>

Since $\omega_N^{-1}$ is also *just another root of unity*, this means that the inverse process of **FFT** is just another **FFT** with an extra step of scaling down the outputs by $N$! Ahhh! The scaling process will definitely take $O(N)$, so if **FFT** can run in $O(N\log{N})$, then the inverse version (**IFFT**) is also the same *(yey)*
<br/>

And it turns out there's an optimized version of **FFT** that allows us to calculate in $O(N\log{N})$! For more details, please consider watching [this video by Reducible](https://youtu.be/h7apO7q16V0?si=KnmopD-iq8Nrtiqq), this guy is really good **:>** 

::ytembed[Reducible's Video]{#h7apO7q16V0}

So now it seems like we've found our path! **However**, if you've watched the video like me, you'd notice there are a *few problems...*
<br/>

# Oh No...

A lot of *oh-noes...*

## Not a Power of 2

In order for this optimized version to work, $N$ **must be a power of 2** *(that's why this is called "Radix-2 FFT")*, and `20000156`, well, is not. We can sort of *"pretending"* that $f(x)$ has more than `20000156` terms, where the leading coefficients are all $0$s.
$$
f(x) = f_0 * x_0 + f_1 * x_1 + ... + f_{N-1} * x_{N-1} \\
\downarrow \\
f(x) = f_0 * x_0 + f_1 * x_1 + ... + f_{N-1} * x_{N-1} + 0 * x_N + 0 * x_{N+1} + 0 * ...
$$
<br/>

## More Outputs

But then that would mean we need more than `20000156` outputs!! We might want to use Lagrange Interpolation to recover more values. But it takes $O(N^2)$!!

Also, in this CTF challenge, we can only get `3` more outputs, because $x \equiv x + 20000159$, so $f(20000160)$ is just $f(1)$... 

*(it turns out, in this particular CTF challenge, we can recover `3` missing values $f(69), f(420), f(1337)$ in $O(4*N)$-ish due to some very clever parameter choices, but that's for later :-))*
<br/>

## No $2^s$-th roots of unity...

**However**, even if we're able to extend to some $2^s$ length, get some $2^s$ outputs, let's say, the closet one $\geq$ `20000156`: $2^{25}$. By then we still wouldn't be able to do **IFFT**: the algorithm requires a $2^{25}$-th root of unity, and no such thing exists in $\mathbb{F}_{p}$! Since prime factors of $p - 1$ are $2, 10000079$, we can only have $2, 10000079$, or $20000158$-th roots of unity only...

You might be tempted to extend the current working field to $\mathbb{F}_{p^k}$, where $p^k - 1$ divides $2^{25}$. But it turns out, up until $k = 1000$, $p^k - 1$ divides $2^{13}$ at best.
<br/>

## Author's Solution

From this point, I keep exhaust myself until the end of the CTF, trying out ideas, but nothing works. If it works, then the runtime is often $O(pq)$ where $q$ is the biggest prime of $p-1$... And then, the author publish their solution in the Discord Server, 

![woah...](discord-solution.png)

... to which `Genni` responds:

![i want it too...](genni-replies.png)

# Bluestein's algorithm / Chirp-Z transform

## Like Multiplying 2 polynomials

Turns out there's a very clever way we can transform the **FFT** problem for $N$ coefficients to another **FFT** problem for $2^s$ coefficients. There's no way a dumb person like me can think stuffs like this. But anyways, here's the explanation: 

This is an another way we can look at the **FFT** matrix multiplication above:
$$
f(\omega_N^{j}) = \sum_{k=0}^{N-1}f_k \text{ } \omega_N^{jk}
$$
<br/>

Since $jk = \frac{j^2 + k^2 - (j-k)^2}{2}$, we have ($\omega_{2N} = \omega_N^{\frac{1}{2}}$):

$$
\begin{matrix}
f(\omega_N^{j})                             & = &                   & \sum_{k=0}^{N-1} & f_k                            & \omega_N^{jk} \\
                                            & = & \omega_{2N}^{j^2} & \sum_{k=0}^{N-1} & f_k                            & \omega_{2N}^{k^2} \text{ } \omega_{2N}^{-(j-k)^2} \\

f(\omega_N^{j}) \text{ } \omega_{2N}^{-j^2} & = &                   & \sum_{k=0}^{N-1} & f_k \text{ } \omega_{2N}^{k^2} & \omega_{2N}^{-(j-k)^2}
\end{matrix}
$$
<br/>

If we let:
- $f(\omega_N^{j}) \text{ } \omega_{2N}^{-j^2}$ be $Z_j$.
- $f_k \text{ } \omega_{2N}^{k^2}$ be $X_k$.
- $\omega_{2N}^{-(j-k)^2}$ be $Y_{j-k}$.
<br/>

We'd obtain the following equation:
$$
\begin{matrix}
Z_j = \sum_{k=0}^{N-1} X_k * Y_{j-k}
\end{matrix}
$$

One may look at this and might relate to the fact that this is like multiplying `2` **"polynomial-ish"** $X(x) * Y(x) = Z(x)$. 
<br/>

Like, given any 2 polynomials $m(x) * n(x) = o(x)$, you would notice that:

$$
\begin{matrix}
o_j = \sum_{k=0}^{j} m_k * n_{j-k}
\end{matrix}
$$

where $m(x) = \sum_{i} m_i x^i, n(x) = \sum_{i} n_i x^i, o(x) = \sum_{i} o_i x^i$. However, there's a difference in the sum: 