---
title: "BRICS+ CTF - SSS (Part 1): We need to Transform..."
date: Tue Oct 04 2023 15:28:12 GMT+0700 (Indochina Time)
description: I'm depressed after trying to solve this...
coverImage: cover.webp
draft: false
---

# Intro

Once in a while, there comes a problem that makes you more conscious about your insignificant intelligence... Today, it comes in the form of a CTF challenge \:'\<\<

Basically, there's this challenge:
- generates a random polynomial $f(x)$ with $N=$ `20000156` terms in $\mathbb{F}_{p}$, where $p=$ `20000159`,
- evaluates the outputs $f(0), f(1), ..., f(p-1)$ except for $f(69), f(420)$ and $f(1337)$,
- gives us said outputs,
- gives us the `FLAG` **XORed** with the `sha512()` hash of $f(x)$'s coefficients array.
<br/>

# Trying stuffs...

Since it's hard to crack `sha512()`, or reverse **XOR** function with 0 knowleges about one of its inputs, our only route is to recover $f(x)$ from its outputs.

## Lagrange Interpolation

So, if you're a noob like me, your first thought will probably be: 
> "Well, this is a piece of cake. 
> 
> We have $20000156$ outputs, the polynomial has $20000156$ terms. I'll just use the [Lagrange Interpolation](https://www.wikiwand.com/en/Lagrange_polynomial). They have this fancy math that **computes a polynomial's $N$ coefficients from its $N$ evaluations**. 
> 
> We meet all the requirements needed to carry out the computation."
<br/>

So you gleefully copy an implementation from the Internet, adding inputs, modify stuffs, typing, thinking to yourself: **"EZ"**. 

You run the code and wonder why it takes so long. Maybe the output array is too big and needed some time to load? No, it had already been done! You throw a `tqdm` bar into the main loop of the code and see that it estimates the algorithm will be running for another `7000` hours.
<br/>

Shit :shit:.

You dig into the implementation of **Lagrange Interpolation**, and realize that given a polynomial with $N$ terms:
- it computes the sum of $N$ terms, 
- each term is the product of smaller $N-1$ linear polynomials.
<br/>

An $O(N^2)$ runtime... 

This algorithm works fast for small polynomial sizes, like `2` or `3` and some numbers close to it like `10000`, but not `20000156`. This is when you **GASP**: *"This is not Crypto, this is some [Codeforces](https://codeforces.com/) obsure **200+ IQ** optimization problem!!"* You know you suck, but you still want try solving a coding optimization stuffs anyways, to see where it goes. Oh boy how shitty you're going feel afterwards!
<br/>

## The Vandermonde Matrix
From your experiences of watching math videos on Youtube, feeling smart then remember like **1%** about it the next day, you realize that ~~you don't know what to do now and feel like shit *(meanwhile, your teammate is saying something about **vandermonde**)*~~ maybe sometimes we need to derail to some random ideas to get to smart one, so that's what you did.
<br/>

So you definitely ~~did look at [this wiki page](https://www.wikiwand.com/en/Vandermonde_matrix) and did not~~ have **an insight** on your own: 

The evaluation of $f(x) = \sum_{k=0}^{N-1} f_k x^k$ at $x_0$ can be represented as a dot product: $f(x_0) = (x_0^0, x_0^1, ..., x_0^{N-1}) \cdot (f_0, f_1, ..., f_{N-1})$.
<br/>

Or maybe, as a multiplication of `1xN` matrix with another `Nx1` matrix?
$$
\begin{bmatrix}
x_0^0 & x_0^1 & ... & x_0^{N-1}
\end{bmatrix}

\begin{bmatrix}
f_0 \\ f_1 \\ ... \\ f_{N-1}
\end{bmatrix}

= 

\begin{bmatrix}
f(x_0)
\end{bmatrix}
$$
<br/>

And you see that you can extend the `1xN` matrix on the left so that on the right side you can compute not only $f(x_0)$, but also $f(x_1), f(x_2), ...$ and so on:
$$
\begin{bmatrix}
x_0^0     & x_0^1     & ... & x_0^{N-1}     \\
x_1^0     & x_1^1     & ... & x_1^{N-1}     \\
... \\
x_{N-1}^0 & x_{N-1}^1 & ... & x_{N-1}^{N-1} \\
\end{bmatrix}

\begin{bmatrix}
f_0 \\ f_1 \\ ... \\ f_{N-1}
\end{bmatrix}

= 

\begin{bmatrix}
f(x_0) \\ f(x_1) \\ ... \\ f(x_{N-1})
\end{bmatrix}
$$
<br/>

The `NxN` matrix on the left side seems like a special type of matrix. Maybe you discovered a new thing ðŸ˜± *(and not something already been discovered and called [the Vandermonde Matrix](https://www.wikiwand.com/en/Vandermonde_matrix))*? Anyways, you decide to name this thing [the Vandermonde Matrix](https://www.wikiwand.com/en/Vandermonde_matrix), because it sounds right somehow...

Let's rewrite the equation into: 
$
V \text{ } \overrightarrow{f} = \overrightarrow{y}
$,
where:
- $\overrightarrow{f}$ is $f(x)$'s coefficients vector.
- $\overrightarrow{y}$ is $f(x)$'s outputs vector.
- $V$ is the Vandermonde matrix.
<br/>

Which leads to $\overrightarrow{f} = V^{-1} \text{ } \overrightarrow{y}$. As long as $x_0 \neq x_1 \neq x_2 \neq ... \neq x_{N-1}$, $V$ is invertible. Anyways, this shifts our focus into...

![this is when I started googling about vandermonde](genni-insight.png)

A couple of Google Searches leads us to [this website](https://proofwiki.org/wiki/Inverse_of_Vandermonde_Matrix):

![seems bad, but I dunno why...](inverse_vandermonde.png)

You have a hunch that this route sucks. Matrix multiplication with a vector still takes $O(N^2)$ as far as you know, and at the same time, `EggRoll`, from your team says this:

![](eggroll-insight.png)

Others seem to brush off his idea, and to be honest, they seem tired and confused at that point, so it's understandable. But you remember from the **1%** of knowledge you watched in some Youtube video, that **FFT** is some algorithm that can be done in $O(N\log{N})$, which is something way better than $O(N^2)$, is probably exactly what we need right now!! 
<br/>

Hmm... But you forgot what it does...
<br/>

## Fast Fourier Transform

**Fast Fourier Transform**, or **FFT** for short, is an algorithm that also can do the **convert coefficients -> values** thingy. However, this algorithm has a restriction: 

The inputs to $f(x)$ must be $\omega_N^0, \omega_N^1, ..., \omega_N^{N-1}$, instead of arbitrary values $x_0, x_1, ..., x_{N-1}$, where $\omega_N$ is **any** $N$-**th [root of unity](https://www.wikiwand.com/en/Root_of_unity)** *(except `1` to avoid $\omega_N^0 \neq \omega_N^1 \neq ... \neq \omega_N^{N-1}$)*. 

> **Hmm..** Actually that's what I understand about this...
> 
> The actual purpose of **FFT** seems to be **some values -> frequency thingy conversion** stuffs. But in this case, it still true **(?)** idk... math is weird... Maybe I'm wrong on something @.@
> 
> But if you want to know the actual definition of **FFT**, you can read it here: [Wiki](https://www.wikiwand.com/en/Fast_Fourier_transform)
<br/>

To understand why **FFT** does this, let's formulize things.

Same as before, this relationship between coefficients and values can be represented as the product of a **Vandermonde matrix** with the **coefficient vector** $\overrightarrow{f}$:

$$
\begin{bmatrix}
    \omega_N^0 & \omega_N^0     & \omega_N^0        & ... & \omega_N^0            & \omega_N^0             \\
    \omega_N^0 & \omega_N^1     & \omega_N^2        & ... & \omega_N^{N-2}        & \omega_N^{N-1}         \\
    \omega_N^0 & \omega_N^2     & \omega_N^4        & ... & \omega_N^{2N-4}       & \omega_N^{2N-2}       \\
    ... \\
    \omega_N^0 & \omega_N^{N-1} & \omega_N^{2(N-1)} & ... & \omega_N^{(N-2)(N-1)} & \omega_N^{(N-1)(N-1)} \\
\end{bmatrix}

\begin{bmatrix}
f_0     \\
f_1     \\
f_2     \\
...     \\
f_{N-2} \\
f_{N-1}
\end{bmatrix}

=

\begin{bmatrix}
f(\omega_N^0) \\
f(\omega_N^1) \\
f(\omega_N^2) \\
... \\
f(\omega_N^{N-2}) \\
f(\omega_N^{N-1})
\end{bmatrix}
$$
<br/>

Let's rewrite the equation into: 
$
D_{\omega_N} \text{ } \overrightarrow{f} = \overrightarrow{y_{\omega_N}}
$, where:
- $\overrightarrow{f}$ is $f(x)$'s coefficients vector.
- $\overrightarrow{y_{\omega_N}}$ is $f(x)$'s outputs vector of $f(x)$ evaluated at $\omega_N^0, \omega_N^1, ..., \omega_N^{N-1}$.
- $D_{\omega_N}$ is a special case of Vandermonde matrix, called the [DFT matrix](https://www.wikiwand.com/en/DFT_matrix), whose entries can be generated solely from $\omega_N$ and their positions.
<br/>

Same as before, we have: $\overrightarrow{f} = D_{\omega_N}^{-1} \text{ } \overrightarrow{y_{\omega_N}}$. And it turns out that,
$$
D_{\omega_N}^{-1} = \frac{D_{\omega_N^{-1}}}{N}
$$
<br/>

Since $\omega_N^{-1}$ is also *just another root of unity*, this means that the inverse process of **FFT** is just another **FFT** with an extra step of scaling down the outputs by $N$! Ahhh! The scaling process will definitely take $O(N)$, so if **FFT** can run in $O(N\log{N})$, then the inverse version (**IFFT**) is also the same *(yey)*
<br/>

And it turns out there's an optimized version of **FFT** that allows us to calculate in $O(N\log{N})$, called the *Radix-2 FFT*! For more details, please consider watching [this video by Reducible](https://youtu.be/h7apO7q16V0?si=KnmopD-iq8Nrtiqq), this guy is really good **:>** 

::ytembed[Reducible's Video]{#h7apO7q16V0}
<br/>

So now it seems like we've found our path! **However**, if you've watched the video like me, you'd notice there are a *few problems...*
<br/>

# Oh No...

A lot of *oh-noes...*

## Not a Power of 2

In order for this optimized version to work, $N$ **must be a power of 2** *(that's why this is called "Radix-2 FFT")*, and `20000156`, well, is not. We can sort of *"pretending"* that $f(x)$ has more than `20000156` terms, where the leading coefficients are all $0$s.
$$
f(x) = f_0 * x_0 + f_1 * x_1 + ... + f_{N-1} * x_{N-1} \\
\downarrow \\
f(x) = f_0 * x_0 + f_1 * x_1 + ... + f_{N-1} * x_{N-1} + 0 * x_N + 0 * x_{N+1} + 0 * ...
$$
<br/>

## Can't get much more outputs

But then that would mean we need more than `20000156` outputs!! We might want to use Lagrange Interpolation to recover more values. But it takes $O(N^2)$!!

Also, in this CTF challenge, we can only get `3` more outputs, because $x \equiv x + 20000159$, so $f(20000160)$ is just $f(1)$... And *none* of the numbers from `20000156` to `20000159` are powers of 2.
<br/>

> **Note from Mistsu in the future**: 
> 
> *It turns out*, in this particular CTF challenge, we can **AND** just need to recover `3` missing values $f(69), f(420), f(1337)$ in $O(4*N)$-ish due to some very clever parameter choices, while still can do **FFT** in $O(N\log{N})$! 
> 
> but we're ahead of ourselves at this point heheheh **:-)**
<br/>

## No $2^s$-th roots of unity...

**However**, even if we're able to extend to some $2^s$ length, get some $2^s$ outputs, let's say, the closet one $\geq$ `20000156`: $2^{25}$. By then we still wouldn't be able to do **IFFT**: the algorithm requires a $2^{25}$-th root of unity, and no such thing exists in $\mathbb{F}_{p}$. 

Because of number theory and [group theory](https://www.wikiwand.com/en/Lagrange%27s_theorem_(group_theory)) stuffs, any subgroup of the [multiplicative group](https://www.wikiwand.com/en/Multiplicative_group) $\mathbb{F}_{p}^{\times}$ must have [order](https://www.wikiwand.com/en/Order_(group_theory)) dividing $p-1$. What this is saying is that good luck trying to find $g \neq 1$ such that $g^{3} \equiv 1 \mod {p}$ when $3 \nmid p-1$...

The only prime factors of $p - 1$ are $2, 10000079$, we can only use $2, 10000079$, or $20000158$-th roots of unity for doing FFT only...
<br/>


...You might be tempted to extend the current working field to $\mathbb{F}_{p^k}$, where you can find subgroups with order dividing $p^k - 1$. **But it turns out**, up until $k = 1000$, $p^k - 1$ is divisible by $2^{13}$ at best. And you might want to stop here, because going further means that your **CPU & RAM** goes bye bye computing those elements. And you also have to do Larange Interpolation to have outputs of inputs in that field...
<br/>

# Author's Solution

From this point, I keep exhaust myself until the end of the CTF, trying out ideas, but nothing works. If it works, then the runtime is often $O(pq)$ where $q$ is the biggest prime of $p-1$... And then, the author publish their solution in the Discord Server, 

![woah...](discord-solution.png)

... to which `Genni` responds:

![i want it too...](genni-replies.png)

After reading this, my mind feels even more clueless... So this post should stop here. Part 2 will be focusing on how to understand the solution \:v
